# 训练流程

<cite>
**本文档中引用的文件**  
- [babygpt_v12_wandb.py](file://babygpt_v12_wandb.py)
- [ci.txt](file://ci.txt)
</cite>

## 目录
1. [数据加载与预处理](#数据加载与预处理)  
2. [模型初始化](#模型初始化)  
3. [训练循环机制](#训练循环机制)  
4. [关键超参数说明](#关键超参数说明)  
5. [评估与验证机制](#评估与验证机制)  
6. [Weights & Biases 集成](#weights--biases-集成)  
7. [模型保存与输出日志解读](#模型保存与输出日志解读)  
8. [常见问题与优化策略](#常见问题与优化策略)

## 数据加载与预处理

该流程首先从文本文件 `ci.txt` 中读取原始中文诗词数据，作为模型训练的语料库。通过 `Tokenizer` 类实现字符级分词，构建词汇表并建立字符与索引之间的映射关系。具体步骤包括：提取文本中所有唯一字符并排序，确定词汇表大小 `vocab_size`，并创建从字符到索引（`stoi`）和从索引到字符（`itos`）的字典。

随后，使用 `encode` 方法将原始文本转换为整数序列张量，并通过 `torch.tensor` 将其加载至指定设备（CPU/GPU）。整个数据集根据 `tain_data_ratio` 划分为训练集和验证集，分别存储于 `data['train']` 和 `data['val']` 中，用于后续的训练与评估。

**Section sources**  
- [babygpt_v12_wandb.py](file://babygpt_v12_wandb.py#L14-L20)  
- [babygpt_v12_wandb.py](file://babygpt_v12_wandb.py#L22-L38)  
- [babygpt_v12_wandb.py](file://babygpt_v12_wandb.py#L145-L147)  
- [ci.txt](file://ci.txt)

## 模型初始化

模型的核心类为 `BabyGPT`，继承自 `nn.Module`。其结构包含以下几个关键组件：

- **嵌入层（Embedding Layer）**：`token_embedding_table` 将输入的 token 索引映射为高维向量空间中的表示。
- **位置编码（Positional Encoding）**：`postion_embedding_table` 为每个位置生成可学习的位置嵌入，以保留序列顺序信息。
- **Transformer 块（Block）**：由多个 `Block` 组成的序列模块，每个 `Block` 包含多头自注意力机制和前馈网络，并结合残差连接与层归一化。
- **多头注意力（MultiHeadAttention）**：将输入拆分为多个“头”，并行计算注意力，最后通过投影层合并输出。
- **前馈网络（FeedFoward）**：包含两个线性层和 ReLU 激活函数，中间维度扩展为 `n_embed * 4`，并加入 Dropout 以防止过拟合。
- **层归一化（LayerNorm）**：在注意力和前馈网络前后分别使用 `nn.LayerNorm`，提升训练稳定性。
- **语言建模头（lm_head）**：最终的线性层，将高维表示映射回词汇表大小维度，用于预测下一个 token。

模型初始化时，通过超参数配置嵌入维度、注意力头数、层数等，构建完整的网络结构。

**Section sources**  
- [babygpt_v12_wandb.py](file://babygpt_v12_wandb.py#L39-L113)  
- [babygpt_v12_wandb.py](file://babygpt_v12_wandb.py#L114-L135)

## 训练循环机制

训练循环通过 `get_batch` 函数从训练数据中随机采样批次。该函数生成 `batch_size` 个起始索引，截取长度为 `block_size` 的序列作为输入 `x`，其对应的下一个 token 序列作为目标 `y`，实现自回归预测任务。

主训练循环中，每一步执行以下操作：
1. 调用 `get_batch` 获取训练批次 `(x, y)`；
2. 前向传播计算 `logits` 和损失 `loss`；
3. 清除梯度（`optimizer.zero_grad(set_to_none=True)`）；
4. 反向传播计算梯度（`loss.backward()`）；
5. 使用 AdamW 优化器更新模型参数（`optimizer.step()`）；
6. 累计已处理的 token 数量，用于性能监控。

整个过程在指定设备上并行执行，充分利用 GPU 加速。

**Section sources**  
- [babygpt_v12_wandb.py](file://babygpt_v12_wandb.py#L148-L155)  
- [babygpt_v12_wandb.py](file://babygpt_v12_wandb.py#L199-L215)

## 关键超参数说明

模型训练过程中涉及多个关键超参数，其设置直接影响模型性能与收敛速度：

- **学习率（learning_rate）**：设为 `3e-4`，适用于 AdamW 优化器，平衡收敛速度与稳定性。
- **批次大小（batch_size）**：设为 `64`，在内存允许范围内提高训练效率。
- **序列长度（block_size）**：设为 `256`，控制上下文窗口大小，影响模型对长距离依赖的捕捉能力。
- **嵌入维度（n_embed）**：设为 `384`，决定 token 表示的丰富程度。
- **注意力头数（n_head）**：设为 `6`，使模型能并行关注不同语义子空间。
- **层数（n_layer）**：设为 `6`，构建深层 Transformer 结构。
- **Dropout 比例（dropout）**：设为 `0.2`，缓解过拟合风险。
- **训练数据比例（tain_data_ratio）**：设为 `0.9`，保留 10% 数据用于验证。

这些参数共同决定了模型容量、训练效率与泛化能力。

**Section sources**  
- [babygpt_v12_wandb.py](file://babygpt_v12_wandb.py#L5-L13)

## 评估与验证机制

为了监控模型在训练集和验证集上的表现，定义了 `estimate_loss` 函数，并使用 `@torch.no_grad()` 装饰器禁用梯度计算，进入评估模式。该函数在不更新参数的前提下，对训练集和验证集分别采样 `eval_iters` 次，计算平均损失。

在主训练循环中，每隔 `eval_interval` 步调用一次 `estimate_loss`，获取当前的训练损失和验证损失，并打印输出。此机制有助于及时发现过拟合或欠拟合现象，确保模型稳定收敛。

**Section sources**  
- [babygpt_v12_wandb.py](file://babygpt_v12_wandb.py#L157-L167)  
- [babygpt_v12_wandb.py](file://babygpt_v12_wandb.py#L217-L225)

## Weights & Biases 集成

项目通过 `wandb.init()` 初始化 Weights & Biases 实验跟踪系统，记录超参数配置与训练过程中的关键指标。每次评估时，调用 `wandb.log()` 将以下信息上传至云端：
- 训练损失（`train_loss`）
- 验证损失（`val_loss`）
- 每秒处理的 token 数量（`tokens_per_sec`）
- 当前训练步数（`iteration`）

这使得用户可以在 Web 界面中实时查看训练曲线、性能趋势和资源利用率，便于实验对比与调优。

**Section sources**  
- [babygpt_v12_wandb.py](file://babygpt_v12_wandb.py#L10-L13)  
- [babygpt_v12_wandb.py](file://babygpt_v12_wandb.py#L224-L226)

## 模型保存与输出日志解读

训练完成后，使用 `torch.save(model.state_dict(), save_path)` 将模型权重保存至 `model.pth` 文件，便于后续加载与推理。

训练过程中输出的日志包含以下信息：
- 当前训练步数（`step {iter}`）
- 训练损失与验证损失（保留四位小数）
- 每秒处理的 token 数量（性能指标）
- 已耗时（分钟+秒）

例如：`step 100: train loss 5.7529, val loss 5.8782, speed: 104971.14 tokens/sec, time: 0m 15.8s`。随着训练进行，训练和验证损失逐渐下降，表明模型正在学习语言规律。

此外，训练结束后会使用预设的 prompts（如“春江”、“往事”）进行文本生成，展示模型的推理能力，并输出生成结果。

**Section sources**  
- [babygpt_v12_wandb.py](file://babygpt_v12_wandb.py#L227-L239)  
- [babygpt_v12_wandb.py](file://babygpt_v12_wandb.py#L240-L254)  
- [babygpt_v12_wandb.py](file://babygpt_v12_wandb.py#L333-L363)

## 常见问题与优化策略

在训练过程中可能遇到以下问题：

- **GPU 内存不足**：可通过减小 `batch_size` 或 `block_size` 缓解；或启用梯度累积策略。
- **梯度爆炸**：使用梯度裁剪（`torch.nn.utils.clip_grad_norm_`）限制梯度范数。
- **训练收敛缓慢**：尝试调整学习率，或使用学习率调度器（如 `CosineAnnealingLR`）。
- **过拟合**：增加 `dropout` 比例，或引入权重衰减（AdamW 已内置）。
- **验证损失波动大**：增大 `eval_iters` 提高评估稳定性。

建议结合 wandb 的可视化功能分析训练动态，针对性地调整超参数或模型结构。

**Section sources**  
- [babygpt_v12_wandb.py](file://babygpt_v12_wandb.py#L199-L215)  
- [babygpt_v12_wandb.py](file://babygpt_v12_wandb.py#L217-L225)