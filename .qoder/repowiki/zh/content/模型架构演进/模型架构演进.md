# 模型架构演进

<cite>
**本文档中引用的文件**  
- [babygpt_v1.py](file://babygpt_v1.py)
- [babygpt_v2_position.py](file://babygpt_v2_position.py)
- [babygpt_v3_self_attention.py](file://babygpt_v3_self_attention.py)
- [babygpt_v4_multihead_attention.py](file://babygpt_v4_multihead_attention.py)
- [babygpt_v5_feedforward.py](file://babygpt_v5_feedforward.py)
- [babygpt_v6_block.py](file://babygpt_v6_block.py)
- [babygpt_v7_residual_connection.py](file://babygpt_v7_residual_connection.py)
- [babygpt_v8_projection.py](file://babygpt_v8_projection.py)
- [babygpt_v9_layer_norm.py](file://babygpt_v9_layer_norm.py)
- [babygpt_v10_dropout.py](file://babygpt_v10_dropout.py)
- [babygpt_v11_hyper_params.py](file://babygpt_v11_hyper_params.py)
- [babygpt_v12_wandb.py](file://babygpt_v12_wandb.py)
</cite>

## 目录
1. [v1：基础嵌入与线性投影](#v1基础嵌入与线性投影)  
2. [v2：引入位置编码](#v2引入位置编码)  
3. [v3：自注意力机制](#v3自注意力机制)  
4. [v4：多头注意力](#v4多头注意力)  
5. [v5：前馈网络](#v5前馈网络)  
6. [v6：模块化设计](#v6模块化设计)  
7. [v7：残差连接](#v7残差连接)  
8. [v8：语言模型头投影层分离](#v8语言模型头投影层分离)  
9. [v9：层归一化](#v9层归一化)  
10. [v10：Dropout正则化](#v10dropout正则化)  
11. [v11：超参数系统化调整](#v11超参数系统化调整)  
12. [v12：集成wandb进行实验跟踪](#v12集成wandb进行实验跟踪)

## v1：基础嵌入与线性投影

BabyGPT模型的初始版本（v1）采用了一个极简的架构，仅包含两个核心组件：词嵌入层和线性投影层。该模型通过`nn.Embedding`将输入的token索引映射到一个低维的连续向量空间（维度由`n_embed`定义），然后直接通过一个`nn.Linear`层将这些嵌入向量映射回词汇表大小的维度，以生成下一个token的概率分布。这种设计虽然简单，但缺乏对序列中token之间依赖关系的建模能力，每个token的预测是孤立的。模型的`forward`方法负责计算损失，而`generate`方法则实现了基于softmax概率分布的随机采样，用于生成新的文本序列。

**本节来源**  
- [babygpt_v1.py](file://babygpt_v1.py#L50-L130)

## v2：引入位置编码

在v2版本中，为了赋予模型理解序列顺序的能力，引入了**位置编码**（Positional Encoding）。由于原始的嵌入层无法区分token在序列中的先后位置，v2版本增加了一个`postion_embedding_table`，它是一个可学习的嵌入层，其大小为`block_size`（序列长度）乘以`n_embed`（嵌入维度）。在前向传播中，token的嵌入表示（`tok_emb`）会与对应位置的嵌入表示（`pos_emb`）相加，从而将位置信息注入到每个token的表示中。这一改进使得模型能够区分“春江”和“江春”这样的不同顺序，是构建序列模型的关键一步。

**本节来源**  
- [babygpt_v2_position.py](file://babygpt_v2_position.py#L50-L137)

## v3：自注意力机制

v3版本是模型演进中的一个重大突破，它引入了**自注意力机制**（Self-Attention）。该机制允许序列中的每个token关注序列中的所有其他token，从而建立全局的依赖关系。在代码中，这通过一个`Head`类实现，该类包含三个线性变换层：`key`、`query`和`value`。通过计算`query`和`key`的点积得到注意力权重，再用这些权重对`value`进行加权求和，最终输出一个融合了全局上下文信息的表示。为了确保模型在生成时只能看到过去的信息（防止信息泄露），使用了下三角矩阵`tril`对注意力权重进行掩码（masked_fill），实现了因果注意力。

**本节来源**  
- [babygpt_v3_self_attention.py](file://babygpt_v3_self_attention.py#L50-L159)

## v4：多头注意力

v4版本将单一的自注意力头扩展为**多头注意力**（Multi-Head Attention）。其核心思想是，不同的注意力头可以学习到序列中不同类型的依赖关系（例如，语法、语义、指代等）。`MultiHeadAttention`类通过创建多个`Head`实例，并将它们的输出在特征维度上拼接起来，从而让模型能够并行地从多个子空间中提取信息。这极大地增强了模型的表征能力。每个头的`head_size`被设置为`n_embed // n_head`，以保证多头注意力的总输出维度与输入维度一致。

**本节来源**  
- [babygpt_v4_multihead_attention.py](file://babygpt_v4_multihead_attention.py#L50-L168)

## v5：前馈网络

在v5版本中，模型在自注意力层之后增加了一个**前馈网络**（Feed-Forward Network, FFN）。FFN是一个简单的两层全连接网络，中间使用ReLU激活函数。它的作用是对自注意力层输出的每个位置的表示进行非线性变换和特征增强。这个组件为模型提供了额外的表达能力，使其能够学习更复杂的函数。FFN通常被设计为先将维度扩展（例如，扩展到4倍），然后再压缩回原始维度。

**本节来源**  
- [babygpt_v5_feedforward.py](file://babygpt_v5_feedforward.py#L50-L180)

## v6：模块化设计

v6版本对代码结构进行了重构，引入了**模块化设计**。通过创建一个`Block`类，将自注意力层（`sa`）和前馈网络（`ffwd`）封装在一起。这不仅使代码更加清晰和易于维护，也为后续堆叠多个相同的处理块（即Transformer层）奠定了基础。`BabyGPT`主模型中的`blocks`属性现在是一个由多个`Block`实例组成的`nn.Sequential`序列，这清晰地体现了模型的深度。

**本节来源**  
- [babygpt_v6_block.py](file://babygpt_v6_block.py#L50-L191)

## v7：残差连接

v7版本引入了**残差连接**（Residual Connection），也称为跳跃连接。在`Block`的`forward`方法中，每一层的输出不再是直接替换输入，而是与输入相加（`x = x + self.sa(x)`）。这种设计对于训练深层网络至关重要，因为它允许梯度在反向传播时直接流过，有效缓解了梯度消失问题。即使中间层的变换效果不佳，原始信息也能通过残差路径传递，保证了网络的稳定性。

**本节来源**  
- [babygpt_v7_residual_connection.py](file://babygpt_v7_residual_connection.py#L50-L191)

## v8：语言模型头投影层分离

v8版本对多头注意力机制进行了完善，明确地添加了一个**投影层**（Projection Layer）。在`MultiHeadAttention`类中，增加了一个`nn.Linear`层`proj`。在v7及之前的版本中，多个头的输出拼接后直接作为结果，这可能导致维度不匹配或信息冗余。v8版本将拼接后的高维向量通过`proj`层线性映射回原始的`n_embed`维度，确保了信息的平滑传递和维度的一致性，这是标准Transformer架构中的一个关键组件。

**本节来源**  
- [babygpt_v8_projection.py](file://babygpt_v8_projection.py#L50-L194)

## v9：层归一化

v9版本在残差连接之后引入了**层归一化**（Layer Normalization）。在`Block`类中，增加了两个`nn.LayerNorm`层（`ln1`和`ln2`）。层归一化被应用于每个token的特征向量上，通过对每个样本的特征进行归一化来稳定训练过程，减少内部协变量偏移。在标准的Transformer实现中，层归一化通常被放置在残差连接之前（即“预归一化”），这有助于进一步稳定深层网络的训练。

**本节来源**  
- [babygpt_v9_layer_norm.py](file://babygpt_v9_layer_norm.py#L50-L198)

## v10：Dropout正则化

为了提升模型的泛化能力，防止过拟合，v10版本系统地应用了**Dropout正则化**。在`Head`、`MultiHeadAttention`和`FeedFoward`类中都添加了`nn.Dropout`层。Dropout在训练过程中随机将一部分神经元的输出置为零，这迫使网络不能过度依赖于某些特定的神经元，从而学习到更鲁棒的特征。`dropout`比率作为一个超参数被定义，通常在0.1到0.5之间。

**本节来源**  
- [babygpt_v10_dropout.py](file://babygpt_v10_dropout.py#L50-L205)

## v11：超参数系统化调整

v11版本标志着模型从架构探索转向了性能优化。该版本对一系列**超参数**进行了系统性的调整，以提升模型的性能。例如，`block_size`从8增加到256，允许模型捕捉更长的上下文依赖；`n_embed`和`n_layer`等维度和层数也显著增加，提升了模型的容量；学习率`learning_rate`被调整为3e-4，这是一个在深度学习中被广泛验证为有效的值。这些调整是模型能够生成更连贯、更高质量文本的关键。

**本节来源**  
- [babygpt_v11_hyper_params.py](file://babygpt_v11_hyper_params.py#L50-L199)

## v12：集成wandb进行实验跟踪

v12是BabyGPT系列的最终版本，它集成了`wandb`（Weights & Biases）库，实现了**实验跟踪和可视化**。通过`wandb.init()`，模型的超参数被记录到一个项目中。在训练循环中，每经过`eval_interval`步，模型的训练损失、验证损失和处理速度等关键指标都会通过`wandb.log()`上传到云端。这使得实验过程完全可追溯，结果可以被直观地可视化和比较，极大地提升了开发和调试的效率。此外，训练完成后模型权重被保存，完成了从训练到部署的闭环。

**本节来源**  
- [babygpt_v12_wandb.py](file://babygpt_v12_wandb.py#L50-L363)