# v9：层归一化

<cite>
**Referenced Files in This Document**   
- [babygpt_v9_layer_norm.py](file://babygpt_v9_layer_norm.py)
</cite>

## 目录
1. [引言](#引言)
2. [层归一化技术概述](#层归一化技术概述)
3. [Block类中的层归一化实现](#block类中的层归一化实现)
4. [前向传播中的预归一化架构](#前向传播中的预归一化架构)
5. [预归一化对深层网络训练的影响](#预归一化对深层网络训练的影响)
6. [结论](#结论)

## 引言
在v9版本中，模型引入了层归一化（Layer Normalization）技术，这是深度神经网络训练稳定性提升的关键改进。本文档将详细阐述这一技术的实现原理、代码结构及其对模型训练的积极影响。

## 层归一化技术概述
层归一化是一种用于稳定神经网络训练过程的技术，其核心作用是减少内部协变量偏移（Internal Covariate Shift）。通过在每个样本的特征维度上计算均值和方差，并据此对数据进行归一化处理，层归一化能够确保每一层的输入分布保持相对稳定。这种稳定性对于深层网络尤为重要，因为它有助于缓解梯度消失或爆炸问题，从而加速模型收敛并提高训练效率。

与批归一化（Batch Normalization）不同，层归一化不依赖于批次统计量，而是针对单个样本的所有特征进行归一化操作。这使得它在处理变长序列或小批次数据时更具优势，特别适用于Transformer架构中的自注意力机制。

## Block类中的层归一化实现
在v9版本中，`Block`类被扩展以包含两个`nn.LayerNorm`层实例：`ln1`和`ln2`。这两个归一化层分别应用于自注意力模块和前馈网络模块之前。

```python
class Block(nn.Module):
    def __init__(self, n_embed, n_head):
        super().__init__()
        head_size = n_embed // n_head
        self.sa = MultiHeadAttention(n_head, head_size)
        self.ffwd = FeedFoward(n_embed)
        self.ln1 = nn.LayerNorm(n_embed) # 自注意力前的归一化层
        self.ln2 = nn.LayerNorm(n_embed) # 前馈网络前的归一化层
```

**Section sources**
- [babygpt_v9_layer_norm.py](file://babygpt_v9_layer_norm.py#L40-L52)

## 前向传播中的预归一化架构
在`forward`方法中，归一化层被巧妙地放置在残差连接之前，形成了所谓的“预归一化”（pre-norm）架构。具体实现如下：

```python
def forward(self, x):
    x = x + self.sa(self.ln1(x)) # 自注意力前先归一化
    x = x + self.ffwd(self.ln2(x)) # 前馈网络前先归一化
    return x
```

这种设计模式的关键在于：输入`x`首先经过`ln1`或`ln2`的归一化处理，然后才进入自注意力（`self.sa`）或前馈网络（`self.ffwd`）模块。计算得到的结果再与原始输入`x`相加，完成残差连接。预归一化的结构确保了传递给每个子模块的数据都具有稳定的分布特性，从而降低了训练过程中的波动性。

**Section sources**
- [babygpt_v9_layer_norm.py](file://babygpt_v9_layer_norm.py#L53-L58)

## 预归一化对深层网络训练的影响
预归一化架构的采用显著促进了深层网络的稳定训练。传统“后归一化”（post-norm）方法将归一化层置于残差连接之后，可能导致梯度信号在深层网络中衰减过快。而预归一化通过在信息流前端施加归一化约束，有效控制了各层激活值的尺度，使得梯度能够更顺畅地反向传播。

此外，预归一化还增强了模型对初始化参数的鲁棒性，减少了对学习率精细调参的需求。实验表明，在相同训练条件下，采用预归一化的模型通常表现出更快的收敛速度和更低的验证损失，尤其是在堆叠多个`Block`时效果更为明显。

## 结论
v9版本通过引入层归一化技术，特别是采用预归一化架构，显著提升了模型的训练稳定性和效率。这一改进不仅体现了对深度学习理论的深入理解，也为后续版本的功能扩展奠定了坚实的基础。未来的工作可以进一步探索其他归一化策略（如RMSNorm）的应用，以持续优化模型性能。